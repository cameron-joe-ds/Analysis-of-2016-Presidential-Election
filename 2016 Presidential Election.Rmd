
---
title: "PSTAT 131: Final project"
author: "Emanuel Medina: 5095906, Razeen Ahmed: 6941736, Cameron Joe: 6114367"
output: pdf_document
---



# Instructions and Expectations

- You are allowed and encouraged to work with two partners on this project.  Include your names, perm numbers, and whether you are taking the class for 131 or 231 credit.

- You are welcome to write up a project report in a research paper format -- abstract, introduction, methods, results, discussion -- as long as you address each of the prompts below.  Alternatively, you can use the assignment handout as a template and address each prompt in sequence, much as you would for a homework assignment.

- There should be no raw R _output_ in the body of your report!  All of your results should be formatted in a professional and visually appealing manner. That means that visualizations should be polished -- aesthetically clean, labeled clearly, and sized appropriately within the document you submit, tables should be nicely formatted (see `pander`, `xtable`, and `kable` packages). If you feel you must include raw R output, this should be included in an appendix, not the main body of the document you submit.  

- There should be no R _codes_ in the body of your report! Use the global chunk option `echo=FALSE` to exclude code from appearing in your document. If you feel it is important to include your codes, they can be put in an appendix.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache = T,
                      fig.align = 'center',
                      warnings= F,
                      fig.height = 4, 
                      fig.width = 4)

library(pander)
library(tidyverse)
library(ggmap)
library(modelr)
library(maps)
library(scales)
library(tidyverse)
library(randomForest)
library(gbm)
library(ROCR)
library(pander)
library(tree)
library(maptree)
library(ggpubr)
library(ggridges)
library(class)
```

# Background

The U.S. presidential election in 2012 did not come as a surprise. Some correctly predicted the outcome of the election correctly including [Nate Silver](https://en.wikipedia.org/wiki/Nate_Silver), 
and [many speculated about his approach](https://www.theguardian.com/science/grrlscientist/2012/nov/08/nate-sliver-predict-us-election).

Despite the success in 2012, the 2016 presidential election came as a 
[big surprise](https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/) 
to many, and it underscored that predicting voter behavior is complicated for many reasons despite the tremendous effort in collecting, analyzing, and understanding many available datasets.

Your final project will be to merge census data with 2016 voting data to analyze the election outcome. 

To familiarize yourself with the general problem of predicting election outcomes, read the articles linked above and answer the following questions. Limit your responses to one paragraph for each.

1. What makes voter behavior prediction (and thus election forecasting) a hard problem?

Voter behavior prediction (and election forecasting) is a difficult problem because voting intention changes over time. Furthermore, polls tend to vary as a result of multiple factors such as sampling error and bias. As a result, it is extremely difficult to accurately predict voter behavior and election results as a whole. 

2. What was unique to Nate Silver's approach in 2012 that allowed him to achieve good predictions?

Silver used hierarchical modeling that accounts for voting behavior on both the national and the state level to achieve highly accurate predictions. In addition, instead of using the most likely voter support percentage as an estimate of actual vote percentages, Silver calculated the probabilities for a range of voter support. Then, using these values, Baye's Theorem, and graph theory, he was able to calculate new likelihoods for a range of support levels. 

3. What went wrong in 2016? What do you think should be done to make future predictions better?

Polls often try to bolster sample size and reduce possible bias and variance by looking to other polls for information. In the case of the 2016 election, small (potentially unavoidable) systematic biases became amplified by this inter-poll reliance which likely caused a much larger trend where polls were systematically off in the same direction - overestimating Hillary's lead over Trump. This could be due to multiple key oversights; it is possible that Trump supporters were reluctant to express their true opinions or distrusted the polls themselves. The unexpected results may also be attributed to lower-than-expected voter turn out or last minute shifts in voter opinions.

One possible way to make polling predictions better may be to reduce the amount of inter-poll reliance. The final paragraph of Bialik and Enten's article expressed a sentiment that polls seem to be incentivized to converge on one opinion or definitive value. Relying less on other polls may introduce bias and variance, but it avoids the risk repeating the mistakes of past polls only to come to a similar conclusion (which isn't that informative at all).


# Data

The `project_data.RData` binary file contains three datasets: tract-level 2010 census data, stored as `census`; metadata `census_meta` with variable descriptions and types; and county-level vote tallies from the 2016 election, stored as `election_raw`.
```{r load_data}
load('project_data.RData')
```

## Election data

Some example rows of the election data are shown below:
```{r}
filter(election_raw, !is.na(county)) %>% 
  head() %>% 
  pander()

```

The meaning of each column in `election_raw` is self-evident except `fips`. The accronym is short for [Federal Information Processing Standard](https://en.wikipedia.org/wiki/FIPS_county_code). In this dataset, `fips` values denote the area (nationwide, statewide, or countywide) that each row of data represent.

Nationwide and statewide tallies are included as rows in `election_raw` with `county` values of `NA`. There are two kinds of these summary rows:

* Federal-level summary rows have a `fips` value of `US`.
* State-level summary rows have the state name as the `fips` value.

4. Inspect rows with `fips=2000`. Provide a reason for excluding them. Drop these observations -- please write over `election_raw` -- and report the data dimensions after removal. 


```{r}
filter(election_raw, fips == 2000) %>% 
  pander(caption = "Observation with fips = 2000")

```


```{r}
election_raw <- election_raw %>%
            filter(fips != 2000) 

dim(election_raw)
```

Observations with a fips of 2000 have no county information on them. In addition, they do not appear to be federal-level data (as their fips value is not "US") and they do not appear to be state-level data (as their fips value is not the state name "AK"). So, observations with a fips value of 2000 are anomylous as they are neither county, state, or federal data. Removing these observations results in an object of dimensions 18345 x 5.


## Census data

The first few rows and columns of the `census` data are shown below.
```{r}
census %>% 
  select(1:6) %>% 
  head() %>% 
  pander(digits = 15)
```
Variable descriptions are given in the `metadata` file. The variables shown above are:
```{r}
census_meta %>% head() %>% pander()
```

\newpage
## Data preprocessing

5. Separate the rows of `election_raw` into separate federal-, state-, and county-level data frames:

    * Store federal-level tallies as `election_federal`.
    
    * Store state-level tallies as `election_state`.
    
    * Store county-level tallies as `election`. Coerce the `fips` variable to numeric.
    
  
```{r}
election_federal <- election_raw %>%
  filter(is.na(county), fips == 'US')

election_state <- election_raw %>%
  filter(is.na(county), fips != 'US')

election <- election_raw %>%
  filter(!is.na(county))

# Shannon County data is off. Likely due to a recent change in fips and name. This fixes the problem.
election_state <- election_state %>% 
  filter(fips != "46102")

shannon <- election_raw %>% 
  filter(fips == 46102) %>% 
  mutate(county = "Shannon County") %>% 
  mutate(fips = 46113)

election <- election %>% 
  rbind(shannon)
```


6. How many named presidential candidates were there in the 2016 election? Draw a bar graph of all votes received by each candidate, and order the candidate names by decreasing vote counts. (You may need to log-transform the vote axis.)


```{r log_votes, fig.height= 5, fig.width= 10}
sorted_federal <- election_federal %>%
  as.data.frame() %>% 
  arrange(desc(votes))

# nrow(sorted_federal)

sorted_federal %>% 
  mutate(candidate = factor(sorted_federal$candidate, levels = sorted_federal$candidate)) %>% 
  ggplot(aes(x = candidate, y = log(votes))) + 
  geom_bar(stat = "identity", fill = "lightblue") + 
  theme(axis.text.x = element_text(angle = 90)) + 
  ggtitle("Log Votes per Candidate") + 
  xlab("Candidates") + 
  ylab("Votes (Log Scaled)") 
```

In total, there were 32 named presidential election candidates in the 2016 race. 


7. Create `county_winner` and `state_winner` by taking the candidate with the highest proportion of votes. (Hint: to create `county_winner`, start with `election`, group by `fips`, compute `total` votes, and `pct = votes/total`. Then choose the highest row using `slice_max` (variable `state_winner` is similar).)

```{r}
county_winner <- election %>%
  group_by(fips) %>%
  mutate(total = sum(votes)) %>%
  mutate(pct = votes /total) %>%
  slice_max(pct)


state_winner <- election_state %>%
  group_by(fips) %>%
  mutate(total = sum(votes)) %>%
  mutate(pct = votes /total) %>%
  slice_max(pct)
```


# Visualization

Here you'll generate maps of the election data using `ggmap`. The .Rmd file for this document contains codes to generate the following map.
```{r state_color, fig.height= 6, fig.width= 10}
states <- map_data("state")

ggplot(states) + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = region, 
                   group = group), 
               color = "white") + 
  coord_fixed(1.3) + # avoid stretching
  guides(fill=FALSE) + # no fill legend
  theme_nothing() # no axes

```

8. Draw a county-level map with `map_data("county")` and color by county.

```{r county_color, fig.height= 6, fig.width= 10}
counties <- map_data("county")

ggplot(counties) + 
  geom_polygon(data = counties, aes(x = long, 
                   y = lat, 
                   fill = subregion, 
                   group = group), 
               color = "white") + 
  coord_fixed(1.3) + # avoid stretching
  guides(fill=FALSE) + # no fill legend
  theme_nothing() # no axes

```

In order to map the winning candidate for each state, the map data (`states`) must be merged with with the election data (`state_winner`).

The function `left_join()` will do the trick, but needs to join the data frames on a variable with values that match. In this case, that variable is the state name, but abbreviations are used in one data frame and the full name is used in the other.


9. Use the following function to create a `fips` variable in the `states` data frame with values that match the `fips` variable in `election_federal`.
```{r, echo = T}
name2abb <- function(statename){
  ix <- match(statename, tolower(state.name))
  out <- state.abb[ix]
  return(out)
}

states <- states %>%
  mutate(fips = name2abb(region))

```

Now the data frames can be merged. `left_join(df1, df2)` takes all the rows from `df1` and looks for matches in `df2`. For each match, `left_join()` appends the data from the second table to the matching row in the first; if no matching value is found, it adds missing values.

10. Use `left_join` to merge the tables and use the result to create a map of the election results by state. Your figure will look similar to this state level [New York Times map](https://www.nytimes.com/elections/results/president). (Hint: use `scale_fill_brewer(palette="Set1")` for a red-and-blue map.)


```{r state_rb, fig.height= 6, fig.width= 10}
state_res <- left_join(states, state_winner)

ggplot(state_res) +
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group), 
                 color = "white") +   
  scale_fill_brewer(palette = "Set1") +
  ggtitle("2016 US State Voting") +
  coord_fixed(1.3) + # avoid stretching
  guides() + # no fill legend
  theme_nothing(legend = T)

```

11. Now create a county-level map. The county-level map data does not have a `fips` value, so to create one, use information from `maps::county.fips`: split the `polyname` column to `region` and `subregion` using `tidyr::separate`, and use `left_join()` to combine `county.fips` with the county-level map data. Then construct the map. Your figure will look similar to county-level [New York Times map](https://www.nytimes.com/elections/results/president).


```{r county_rb, fig.height= 6, fig.width= 10}
county_fips <- separate(maps::county.fips, polyname, c("region", "subregion"), sep = ",")

county_joined <- left_join(counties, county_fips)
county_joined <- county_joined %>% 
  mutate(fips = as.character(fips)) # This join will break if fips was converted to numeric earlier
county_joined_2 <- left_join(county_joined, county_winner)

ggplot(county_joined_2) +
  geom_polygon(aes(x = long,
                   y = lat,
                   fill = candidate,
                   group = group),
               color = "white") +
  coord_fixed(1.3) + # avoid stretching
  scale_fill_brewer(palette = "Set1") +
  ggtitle("2016 US County Voting") +
  theme_nothing(legend = T) # no axes

```

12. Create a visualization of your choice using `census` data. Many exit polls noted that [demographics played a big role in the election](https://fivethirtyeight.com/features/demographics-not-hacking-explain-the-election-results/). If you need a starting point, use [this Washington Post article](https://www.washingtonpost.com/graphics/politics/2016-election/exit-polls/) and [this R graph gallery](https://www.r-graph-gallery.com/) for ideas and inspiration.

```{r eth_hist, fig.height= 3, fig.width= 10}
# Ethnicity plot 
ethnic_pct <- c("Pacific", "Asian", "Native", "White", "Hispanic")

census %>% 
  na.omit() %>% 
  subset(select = names(census) %in% c(ethnic_pct, "CensusTract")) %>% 
  gather(key = 'variables', value = 'values', -CensusTract) %>% 
  ggplot(aes(x = values)) +  
  geom_histogram(bins = 20, fill = "lightblue") + 
  ggtitle("Histograms of Ethnicity Percents Across Subcounties") + 
  scale_color_brewer(palette = "Dark2") + 
  facet_wrap(~variables, ncol = 5) + 
  ylab("Counts") + 
  xlab("Percent of People") + 
  theme_bw()
```

Looking at the histograms, it can be seen that those of Pacific islander and native american ethnicity often make up less than 10% of the population. Asian and Hispanic ethnicities make up a larger percent of the population more often - usually within the range of 0 - 25%. White people consist of a majority most often. Frequently exceeding 75% of the population and sometimes even close to 100% of the population.

```{r job_hist, fig.height= 3, fig.width= 10}
# job plot 
job_pct <- c("Drive", "Production", "Construction", "Office", "Service", "Professional")

census %>% 
  na.omit() %>% 
  subset(select = names(census) %in% c(job_pct, "CensusTract")) %>% 
  gather(key = 'variables', value = 'values', -CensusTract) %>% 
  ggplot(aes(x = values)) +  
  geom_histogram(bins = 20, fill = "lightblue") + 
  ggtitle("Histograms of Profession Percents Across Subcounties") + 
  scale_color_brewer(palette = "Dark2") + 
  facet_wrap(~variables, ncol = 6) + 
  ylab("Counts") + 
  xlab("Percent of People in the Profession") + 
  theme_bw()
```

Construction, service, and production related jobs seem to consist of around 10% of the population jobs. Professional and office jobs are most commonly observed to consist of around 25% of the population jobs. Surprisingly, driving jobs are most commonly observed at consisting of 80% of population jobs.

```{r pop_hist, fig.height= 3, fig.width= 10}
# Population plots 
people <- c("Men", "Women", "TotalPop")

census %>% 
  na.omit() %>% 
  subset(select = names(census) %in% c(people, "CensusTract")) %>% 
  mutate(Men = Men/TotalPop,
         Women = Women/TotalPop) %>%  
  gather(key = 'variables', value = 'values', -CensusTract) %>% 
  ggplot(aes(x = values)) +  
  geom_histogram(bins = 20, fill = "lightblue") + 
  ggtitle("Histograms of Population Parameters Across Subcounties") + 
  scale_color_brewer(palette = "Dark2") + 
  facet_wrap(~variables, ncol = 6, scales = "free") + 
  ylab("Counts") + 
  xlab("Percent of Gender/Total Population Count") + 
  theme_bw()
```

Both men and women are most commonly seen to consist of 50% of the population (with men more commonly observed as being under 50% and women more commonly observed as being over 50%).
   
```{r trans_hist, fig.height= 3, fig.width= 10}
# Transporation plots 
transport_pct <- c("Drive", "Carpool", "Transit", "Walk", "OtherTransp")

census %>% 
  na.omit() %>% 
  subset(select = names(census) %in% c(transport_pct, "CensusTract")) %>% 
  gather(key = 'variables', value = 'values', -CensusTract) %>% 
  ggplot(aes(x = values)) +  
  geom_histogram(bins = 20, fill = "lightblue") + 
  ggtitle("Histograms of Transportation Percentages Across Subcounties") + 
  scale_color_brewer(palette = "Dark2") + 
  facet_wrap(~variables, ncol = 6) + 
  ylab("Counts") + 
  xlab("Percent of People that use the Transportation Method") + 
  theme_bw()
```

Walking, other transportation, and transit are most commonly observed at consisting of around 0% of the population transportation. Carpool appears to be seen at higher percentages more frequently, with its mode being at around 10%. Driving is observed at higher frequencies most often with its mode being at around 80%. 

```{r income_hist, fig.height= 3, fig.width= 10}
# Income plots
incomes <- c("IncomePerCapErr", "IncomePerCap", "IncomeErr", "Income")

census %>% 
  na.omit() %>% 
  subset(select = names(census) %in% c(incomes, "CensusTract")) %>% 
  gather(key = 'variables', value = 'values', -CensusTract) %>% 
  ggplot(aes(x = values)) +  
  geom_histogram(bins = 20, fill = "lightblue") + 
  ggtitle("Histograms of Income Variables Across Subcounties") + 
  scale_color_brewer(palette = "Dark2") + 
  facet_wrap(~variables, ncol = 6, scales = "free") + 
  ylab("Counts") + 
  xlab("Dollars") +
  scale_x_continuous(labels = comma) + 
  theme_bw()

# Unused numeric variables 
unused <- c("MeanCommute", "Unemployment", "FamilyWork", "SelfEmployed", "PrivateWork", "Employed", "WorkAtHome",
              "ChildPoverty", "Poverty", "Citizens")
```

The most frequently observed average income is seen to be around 50,000.

13. The `census` data contains high resolution information (more fine-grained than county-level). Aggregate the information into county-level data by computing population-weighted averages of each attribute for each county by carrying out the following steps:
    
* Clean census data, saving the result as `census_del`: 
  
   + filter out any rows of `census` with missing values;
   + convert `Men`, `Employed`, and `Citizen` to percentages;
   + compute a `Minority` variable by combining `Hispanic`, `Black`, `Native`, `Asian`, `Pacific`, and remove these variables after creating `Minority`; and
   + remove `Walk`, `PublicWork`, and `Construction`.
 
 
* Create population weights for sub-county census data, saving the result as `census_subct`: 
    + group `census_del` by `State` and `County`;
    + use `add_tally()` to compute `CountyPop`; 
    + compute the population weight as `TotalPop/CountyTotal`;
    + adjust all quantitative variables by multiplying by the population weights.
    
* Aggregate census data to county level, `census_ct`: group the sub-county data `census_subct` by state and county and compute popluation-weighted averages of each variable by taking the sum (since the variables were already transformed by the population weights)
    
* Print the first few rows and columns of `census_ct`. 


```{r}
census_del <- census %>%
  na.omit() %>%
  mutate(Men = (Men / TotalPop) * 100) %>%
  mutate(Employed = (Employed / TotalPop) * 100) %>%
  mutate(Citizen = (Citizen / TotalPop) * 100) %>%
  mutate(Minority = Hispanic + Black + Native + Asian + Pacific) %>%
  select(-Hispanic, -Black, -Native, -Asian, -Pacific, -Walk, -PublicWork, -Construction)
  
census_subct <- census_del %>%
  group_by(State, County) %>%
  add_tally(TotalPop, name = "CountyPop") %>%
  mutate(pop_weight = TotalPop / CountyPop)

census_subct[, 5:30] <- census_subct[,5:30]*census_subct$pop_weight

census_ct <- census_subct %>%
  group_by(State, County) %>%
  summarise(across(Men:Minority, sum))

census_ct %>% 
  select(1:5) %>% 
  head() %>% 
  pander(digits = 6, caption = "First Few Rows and Columns of Census_ct")
```

14. If you were physically located in the United States on election day for the 2016 presidential election, what state and county were you in? Compare and contrast the results and demographic information for this county with the state it is located in. If you were not in the United States on election day, select any county. Do you find anything unusual or surprising? If so, explain; if not, explain why not.

```{r}
census_state <- census_subct %>%
  group_by(State, County) %>%
  summarise(across(Men:Minority, sum), across(CountyPop, mean)) %>% 
  group_by(State) %>%
  add_tally(CountyPop, name = "StatePop") %>%
  mutate(pop_weight = CountyPop / StatePop)

census_state[, 3:28] <- census_state[, 3:28]* census_state$pop_weight

census_state <- census_state %>%
  group_by(State) %>%
  summarise(across(Men:Minority, sum), across(StatePop, mean))

census_state %>% 
  filter(State == "California") %>% 
  pander(caption = "California Census Information")

census_ct %>% 
  filter(County == "San Diego") %>% 
  pander(caption = "San Diego Census Information")
```

For the 2016 election, one of the group members was in California, specifically the county of San Diego. It is surprising to see that only 66% percent of the population were citizens at the time, but that statistic is not odd considering California has a 63.0% citizen percent. In addition, the average for San Diego income was \$69,943.33 which intuitively feels high, but again is not different from the Californian average of \$67,908.21. The average percent of people that worked at home was a measly 6.32% for the county and 5.17% for the state (this number has probably drastically increased in the current times). In addition, I was very surprised to see that 47% of the population was white in San Diego while only 38.72% of the population was white for California as a whole. This surprised me cause I had assumed that San Diego was more eclectic than California. Looking at information from the census bureau, it appears that the class of White sometimes includes those of Hispanic and Latino ethnicities (which explains why some sources report percents of up to 72% white population for California). This dataset appears to separate the two ethnicities definitively.

# Exploratory analysis

15. Carry out PCA for both county & sub-county level census data. Compute the first two principal components PC1 and PC2 for both county and sub-county respectively. Discuss whether you chose to center and scale the features and the reasons for your choice. Examine and interpret the loadings.

```{r county_PCA, fig.height= 4, fig.width= 10}
#county
x_mx <- census_ct

x_mx <- x_mx[3:28]

x_mx <- x_mx %>% scale(center = T, scale = T)


x_svd <- svd(x_mx)

#loadings
v_svd <- x_svd$v[,1:2]
colnames(v_svd) <- paste('PC', 1:2, sep = '')

#PC's 
z_ctr <- data.frame(as.matrix(x_mx %*% v_svd))
colnames(z_ctr) <- paste('PC', 1:2, sep = '')
z_ctr %>% head() %>% pander(caption = "Head of County PCA Loadings")

v_svd %>%
  as.data.frame() %>%
  mutate(variable = colnames(x_mx)) %>%
  gather(key = 'PC', value = 'Loading', 1:2) %>% arrange(variable) %>%
  ggplot(aes(x = variable, y = Loading)) + geom_point(aes(shape = PC))+
  theme_bw()+
  geom_hline(yintercept = 0, color = 'purple')+
  geom_path(aes(linetype = PC, group = PC,col = PC)) +
  theme(axis.text.x = element_text(angle = 90)) + 
  labs(x = '') + 
  xlab("Variables") + 
  ylab("Loadings") + 
  ggtitle("County PCA Loadings")

```

Single value decomposition is dependent on squared error loss and as a result, is dependent upon variable scale. So, we chose to scale variables before conducting principle component analysis. PC1 up-weights Child Poverty, Minority, Poverty, Service and Unemployment the most, and down weights Employed, Income, IncomePerCap, Professional and White. Therefore, PC1 mostly represents the employment/income/minority/or not components, and a high PC1 corresponds to high poverty and likelihood of being a minority while a low PC1 corresponds to higher level employment/income/likelihood to be white. PC2 up-weights FamilyWork, Men, SelfEmployed, and Work-At-Home the most, and down weights Women, PrivateWork, Drive, Office, and IncomeErr. So, PC2 represents sex and what someone’s job is like. High PC2 means high likelihood of being a man, self employed, and working from home, and vice versa.

```{r subcounty_PCA, fig.height= 4, fig.width= 10}
#sub county
x_mx1 <- census_subct

x_mx1 <- x_mx1[4:32]

x_mx1 <- x_mx1 %>% scale(center = T, scale = T)


x_svd1 <- svd(x_mx1)

#loadings
v_svd1 <- x_svd1$v[,1:2]
colnames(v_svd1) <- paste('PC', 1:2, sep = '')

#PC's 
z_ctr1 <- data.frame(as.matrix(x_mx1 %*% v_svd1))
colnames(z_ctr1) <- paste('PC', 1:2, sep = '')
z_ctr1 %>% head() %>% pander(caption = "Head of Subcounty PCA Loadings")

v_svd1 %>%
  as.data.frame() %>%
  mutate(variable = colnames(x_mx1)) %>%
  gather(key = 'PC', value = 'Loading', 1:2) %>% arrange(variable) %>%
  ggplot(aes(x = variable, y = Loading)) + geom_point(aes(shape = PC))+
  theme_bw()+
  geom_hline(yintercept = 0, color = 'purple')+
  geom_path(aes(linetype = PC, group = PC,col = PC)) +
  theme(axis.text.x = element_text(angle = 90)) + 
  labs(x = '') + 
  xlab("Variables") + 
  ylab("Loadings") + 
  ggtitle("Subcounty PCA Loadings")
```

PC1 seems to upweight all variables except for TotalPop and CountyPop, which are weighted nearly neutrally. Of the up-weighted variables, Family Work, Minority, OfficeTransport and Transit are weighted the least. PC2 mostly up-weights FamilyWork, SelfEmployed, White, and WorkAtHome, and mostly down weights ChildPoverty, Minority, Poverty and Unemployment. So a high PC2 represents high rates of SelfEmployment, Whiteness, FamilyWork and WorkAtHome and low rates of Minority-hood, Unemployment and Poverty, and the opposite is true for low PC2.

16. Determine the minimum number of PCs needed to capture 90% of the variance for both the county and sub-county analyses. Plot the proportion of variance explained and cumulative variance explained for both county and sub-county analyses.


```{r county_PCA_plot, fig.height= 4, fig.width= 10}
#county
pcV <- x_svd$d^2/(nrow(x_mx) - 1)

# scree and cumulative variance plots
tibble(PC = 1:min(dim(x_mx)),Proportion = pcV/sum(pcV),Cumulative = cumsum(Proportion)) %>%
 gather(key = 'measure', value = 'Variance Explained', 2:3) %>%
 ggplot(aes(x = PC, y = `Variance Explained`) ) +
 geom_point() +
 geom_path() +
 facet_wrap(~ measure) +
 theme_bw() +
 theme(axis.ticks.length=unit(.25, "cm")) +
 scale_y_continuous(breaks = seq(0, 1.00, 0.05)) +
 scale_x_continuous(breaks=seq(0, 30, 2)) +
 geom_hline(yintercept = 0.9, color = 'red') + 
  ggtitle("County PCA-Variance Plots")
```

Looking at the cumulative variance plot, it can be seen that 13 PCs are required to capture 90% of the variance of the county data. 

```{r subcounty_PCA_plot, fig.height= 4, fig.width= 10}
#sub county
pcV <- x_svd1$d^2/(nrow(x_mx1) - 1)

# scree and cumulative variance plots
tibble(PC = 1:min(dim(x_mx1)),Proportion = pcV/sum(pcV),Cumulative = cumsum(Proportion)) %>%
 gather(key = 'measure', value = 'Variance Explained', 2:3) %>%
 ggplot(aes(x = PC, y = `Variance Explained`) ) +
 geom_point() +
 geom_path() +
 facet_wrap(~ measure) +
 theme_bw() +
 theme(axis.ticks.length=unit(.25, "cm")) +
 scale_y_continuous(breaks = seq(0, 1.00, 0.05)) +
 scale_x_continuous(breaks=seq(0, 30, 2)) +
 geom_hline(yintercept = 0.9, color = 'red') + 
  ggtitle("Subcounty PCA-Variance Plots")
```

Looking at the cumulative variance plot, it can be seen that 6 PCs are required to capture 90% of the variance of the subcounty data.

17. With `census_ct`, perform hierarchical clustering with complete linkage.  Cut the tree to partition the observations into 10 clusters. Re-run the hierarchical clustering algorithm using the first 5 principal components the county-level data as inputs instead of the original features. Compare and contrast the results. For both approaches investigate the cluster that contains San Mateo County. Which approach seemed to put San Mateo County in a more appropriate cluster? Comment on what you observe and discuss possible explanations for these observations.


```{r}
d_mx <- dist(census_ct, method = 'euclidean')
hclust_out <- hclust(d_mx, method = 'complete')
clusters <- cutree(hclust_out, k = 10) %>% 
  factor(labels = paste('cluster', 1:10))

tibble(clusters) %>% 
  count(clusters) %>%
  pander(caption = "Hierarchical Clustering Counts on Census Variables")

#perform using 5 principal components
#distances between points
v_svd2 <- x_svd$v
z_ctr5 <- data.frame(as.matrix(x_mx %*% v_svd2))
colnames(z_ctr5) <- paste('PC', 1:5, sep = '')
d_mx2 <- dist(z_ctr5[,1:5], method = 'euclidean')

#compute hierarchical cluserting
hclust_out2 <- hclust(d_mx2, method = 'complete')

#cut the tree to partition
clusters2 <- cutree(hclust_out2, k = 10) %>%
  factor(labels = paste('cluster', 1:10))

tibble(clusters2) %>% 
  count(clusters2) %>%
  rename(clusters = clusters2) %>%
  pander(caption = "Hierarchical Clustering Counts on first 5 PCs")


sm5_clusters <- clusters2[which(census_ct$County == "San Mateo")]
sm5_index <- which(clusters == sm5_clusters)

smOG_clusters <- clusters[which(census_ct$County == "San Mateo")]
smOG_index <- which(clusters == smOG_clusters)

data.frame(Data = c("Original Features", "First 5 PCs"),
           Cluster = c(smOG_clusters,sm5_clusters),Mean = c(mean(d_mx[smOG_index]), mean(d_mx2[sm5_index])),San_Mateo_Distance = c(d_mx[which(census_ct$County == "San Mateo")],d_mx2[which(census_ct$County == "San Mateo")]), Variance = c(var(d_mx[smOG_index]), var(d_mx2[sm5_index]))) %>% 
  pander(caption = "Comparing San Mateo Clustering")
```

The original features place San Mateo County in cluster 7. In constrast, the first 5 PCs place San Mateo County in cluster 3. The first 5 PCs approach appears to put San Mateo County in a more appropriate cluster.


# Classification

In order to train classification models, we need to combine `county_winner` and `census_ct` data. This seemingly straightforward task is harder than it sounds. Codes are provided in the .Rmd file that make the necessary changes to merge them into `election_cl` for classification.

```{r}
abb2name <- function(stateabb){
  ix <- match(stateabb, state.abb)
  out <- tolower(state.name[ix])
  return(out)
}

tmpwinner <- county_winner %>%
  ungroup %>%
  # coerce names to abbreviations
  mutate(state = abb2name(state)) %>%
  # everything lower case
  mutate(across(c(state, county), tolower)) %>%
  # remove county suffixes
  mutate(county = gsub(" county| columbia| city| parish", 
                       "", 
                       county)) 

tmpcensus <- census_ct %>% 
  ungroup %>%
  mutate(across(c(State, County), tolower))

election_county <- tmpwinner %>%
  left_join(tmpcensus, 
            by = c("state"="State", "county"="County")) %>% 
  na.omit()

## save meta information
election_meta <- election_county %>% 
  select(c(county, fips, state, votes, pct, total))

## save predictors and class labels
election_county <- election_county %>% 
  select(-c(county, fips, state, votes, pct, total))
```
After merging the data, partition the result into 80% training and 20% testing partitions.

```{r}
set.seed(420) 

elect_ct_part <- resample_partition(election_county, c(test = 0.2, train = 0.8))
train <- as_tibble(elect_ct_part$train)
test <- as_tibble(elect_ct_part$train)
```

18. Decision tree: train a decision tree on the training partition, and apply cost-complexity pruning. Visualize the tree before and after pruning. Estimate the misclassification errors on the test partition, and intepret and discuss the results of the decision tree analysis. Use your plot to tell a story about voting behavior in the US (see this [NYT infographic](https://archive.nytimes.com/www.nytimes.com/imagepages/2008/04/16/us/20080416_OBAMA_GRAPHIC.html)).

```{r init_tree, fig.height= 10, fig.width= 16}
nmin <- 5
tree_opts <- tree.control(nobs = nrow(train), minsize = nmin,mindev = exp(-8)) 
t_0 <- tree(as.factor(candidate) ~ ., data = train, control = tree_opts, split = 'deviance')


draw.tree(t_0, cex = 0.4, size = 2 ,digits = 2)
```


```{r pruned_tree, fig.height= 3, fig.width= 10}
set.seed(2021)
#cost-complexity pruning
nfolds <- 10
cv_out <- cv.tree(t_0, K = nfolds, method = 'deviance')

cv_df <- tibble(alpha = cv_out$k, impurity = cv_out$dev,size = cv_out$size)

best_alpha <- slice_min(cv_df, impurity) %>% slice_min(size)

t_opt <- prune.tree(t_0, k = best_alpha$alpha)

draw.tree(t_opt, cex = 0.62, pch = 2, size = 2, digits = 2)
```

Looking at the pruned tree, it appears that the percent of the population that utilizes public transportation is one of the most important variables as it is split upon the earliest and twice. It appears that increased percent of public transportation use favors Hillary Clinton. In addition, increased percent minority and increased population unemployment favors Hillary Clinton. In contrast, an increased white population percent favors Donald Trump.

```{r tree_error}
# misclass errors 
can.pred.test <- predict(t_opt, newdata = test, type = "vector") 

recalib <- factor(can.pred.test[, 2] > 0.5, labels = c('Donald Trump','Hillary Clinton')) 
errors <- table(class = test$candidate,pred = recalib) 
errors <- errors / rowSums(errors)
errors %>% 
  pander(caption = "Pruned Tree Misclassification Error")
```

19. Train a logistic regression model on the training partition to predict the winning candidate in each county and estimate errors on the test partition. What are the significant variables? Are these consistent with what you observed in the decision tree analysis? Interpret the meaning of one or two significant coefficients of your choice in terms of a unit change in the variables. Did the results in your particular county (from question 14) match the predicted results?  

```{r}
#### CHANGE ####
fit_glm <- glm(as.factor(candidate) ~ ., family = "binomial", data = train)

p_hat_glm <- predict(fit_glm, test, type = "response")
y_hat_glm <- factor(p_hat_glm >0.5, labels=c('Donald Trump', 'Hillary Clinton'))

errors <- table(test$candidate, y_hat_glm)
errors <- errors / rowSums(errors)
errors %>%
   pander(caption = "Logist Model Misclassification Error")


### Looking at the model 
coef_names <- attr(fit_glm$terms, "term.labels")
model_coef <- summary(fit_glm)$coefficients[,c(1,4)]%>% 
  as.data.frame() %>% 
  mutate(Variables = c("Intercept", coef_names))

model_coef %>% 
  filter(`Pr(>|z|)` < 0.05) %>%
  arrange(`Pr(>|z|)`) %>% 
  mutate(Exponential_coef = exp(Estimate)) %>%
  relocate(Variables, .before = Estimate) %>% 
  relocate(Exponential_coef, .after = Estimate) %>% 
  pander(caption = "Logistic Model Significant variables") 
```

In general, the significant variables in our logistic model do not match those found in the pruned tree model from the prior question. The variables white, transit, and minority were all significant in the pruned tree, but are absent from the logistic regression. Unemployment was the only variable shared between the two. It is very possible that variables found significant in the tree and unsignificant in the logistic model covaried with other variables found significant in the logistic model and so their variance was explained indirectly through other variables. A one percent increase in people in the service job field is associated with an increase in the odds of voting for Hillary Clinton by a factor of 1.401. 

```{r}
SD <- tmpwinner %>%
  left_join(tmpcensus, by = c("state"="State", "county"="County")) %>% 
  na.omit() %>% 
  filter(county == "san diego"| county == "orange") %>% # Some weirdness here. I needed to include multiple observations (with both factor levels) for the factor function to work. No big deal, just might look weird.
  select(-c(fips, state, votes, pct, total))

SD_pred <- predict(fit_glm, SD, type = "response")
SD_y_hat <- factor(SD_pred > 0.5, labels = c('Donald Trump', 'Hillary Clinton'))[9]

SD %>% 
  filter(county == "san diego") %>%
  mutate(predicted_candidate = SD_y_hat) %>% 
  relocate(predicted_candidate, .after = candidate) %>% 
  pander(caption = "Predicted vs Actual Candidate for San Diego 2016")
```

Our model correctly predicted that Hillary Clinton would be the winning candidate for San Diego county.


20.  Compute ROC curves for the decision tree and logistic regression using predictions on the test data, and display them on the same plot. Based on your classification results, discuss the pros and cons of each method. Are the different classifiers more appropriate for answering different kinds of questions about the election?


```{r}
can.pred.test <- predict(t_opt, newdata = test, type = "vector") 

topt_prediction <- prediction(predictions = can.pred.test[, 2], labels = test$candidate)

topt_perf <- performance(topt_prediction, 'tpr', 'fpr') 
rate_df <- tibble(tpr = slot(topt_perf, 'y.values'), fpr = slot(topt_perf, 'x.values'),
alpha = slot(topt_perf, 'alpha.values')) %>% unnest(everything()) %>%
mutate(youden = tpr - fpr)

optimal_thresh <- rate_df %>% slice_max(youden)


roc_tree <- rate_df %>%
  ggplot(aes(x = fpr, y = tpr)) + 
  geom_path(color = 'dodgerblue') +
  geom_point(data=optimal_thresh, color= 'red') + 
  theme_bw()
```


```{r}
p_hat_glm <- predict(fit_glm, test, type = "response")

election_predict <- prediction(predictions = p_hat_glm, labels = test$candidate)

# compute error rates as a function of the probability threshold
perf_log <- performance(prediction.obj = election_predict, 'tpr', 'fpr')

# extract rates and threshold from perf_lda as a tibble
rates_log <- tibble(fpr = perf_log@x.values,tpr = perf_log@y.values,
thresh = perf_log@alpha.values) %>%
unnest(everything())

# compute youden's stat
rates_log <- rates_log %>% mutate(youden = tpr - fpr)

# find the optimal value
optimal_thresh <- rates_log %>% slice_max(youden)


roc_reg <- rates_log %>%
  ggplot(aes(x = fpr, y = tpr)) + 
  geom_path(color = 'dodgerblue') +
  geom_point(data=optimal_thresh, color= 'red') + 
  theme_bw()
```

```{r,fig.height= 5, fig.width= 10}
ggarrange(roc_reg, roc_tree, labels = c('Logistic Regression ROC', 'Decision Tree ROC'), ncol=2, nrow=1)
```

One of the biggest advantages of logistic regression is interpretability. With logistic regression coefficients, you can quantitavely explain the association between predictors and class label probabilities. Logistic modeling can also help identify important predictors, as seen in question 19. However, any logistic regression model is limited as it has assumed functional form and is increasingly more difficult to extend beyond two categories. Decision trees offer clear visualization and are very intuitive as seen with the prune tree in question 18. Another advantage is that decision trees do not require normalization or scaling of data. However, a small change in the data can cause a large change in the structure of the decision tree causing instability or overfitting. In addition, the logic behind trees does not often fit the way things happen in the real world - people or events do not behave according to hard boundary lines. 

# Taking it further

21. This is an open question. Interpret and discuss any overall insights gained in this analysis and possible explanations. Use any tools at your disposal to make your case: visualize errors on the map, discuss what does or doesn't seem reasonable based on your understanding of these methods, propose possible directions (for example, collecting additional data or domain knowledge).  In addition, propose and tackle _at least_ one more interesting question. Creative and thoughtful analyses will be rewarded! 

Some possibilities for further exploration are:

  * Data preprocessing: we aggregated sub-county level data before performing classification. Would classification at the sub-county level before determining the winner perform better? What implicit assumptions are we making?

  * Exploring one or more additional classification methods: KNN, LDA, QDA, random forest, boosting, neural networks. (You may research and use methods beyond those covered in this course). How do these compare to logistic regression and the tree method?

  * Use linear regression models to predict the `total` vote for each candidate by county.  Compare and contrast these results with the classification models.  Which do you prefer and why?  How might they complement one another?
    
  * Conduct an exploratory analysis of the "purple" counties-- the counties which the models predict Clinton and Trump were roughly equally likely to win.  What is it about these counties that make them hard to predict?
    
  * Instead of using the native attributes (the original features), we can use principal components to create new (and lower dimensional) sets of features with which to train a classification model. This sometimes improves classification performance.  Compare classifiers trained on the original features with those trained on PCA features. 
  
  
```{r, fig.height= 10, fig.width= 10}
color_class <- function(percent_difference, candidate) {
  if (abs(percent_difference) <= 0.04) {
    color = "Purple"
  } else if (candidate == "Donald Trump") {
    color = "Red"
  } else {
    color = "Blue"
  }
  color
}


colors <- election %>% 
  group_by(fips) %>% 
  mutate(total = sum(votes)) %>% 
  mutate(first = max(votes)) %>% 
  mutate(second = dplyr::nth(votes, 2)) %>% 
  mutate(percent_dif = (first - second)/total) %>% 
  slice_max(votes) %>% 
  mutate(color = color_class(percent_dif, candidate)) %>% 
  mutate(color = as.factor(color))
```

```{r, fig.height= 6, fig.width= 10}
### Plotting map
county_colors <- left_join(county_joined, colors)

ggplot(county_colors) +
  geom_polygon(aes(x = long,
                   y = lat,
                   fill = color,
                   group = group),
               color = "white") +
  coord_fixed(1.3) + # avoid stretching
  scale_fill_manual(values = c("red", "steelblue", "orchid4")) +
  ggtitle("2016 US County Voting") +
  theme_nothing(legend = T) # no axes
```

For the above map, purple states were defined as having less than a 4% total vote difference between the top two candidates for the county.

```{r}
### Joining
abb2name <- function(stateabb){
  ix <- match(stateabb, state.abb)
  out <- tolower(state.name[ix])
  return(out)
}


tmpcolors <- colors %>%
  ungroup %>%
  # coerce names to abbreviations
  mutate(state = abb2name(state)) %>%
  # everything lower case
  mutate(across(c(state, county), tolower)) %>%
  # remove county suffixes
  mutate(county = gsub(" county| columbia| city| parish", 
                       "", 
                       county)) 

tmpcensus <- census_ct %>% 
  ungroup %>%
  mutate(across(c(State, County), tolower))

election_colors <- left_join(tmpcensus, tmpcolors,
            by = c("State" = "state", "County" = "county")) %>% 
  na.omit()

## save meta information
colors_meta <- election_colors %>% 
  select(c(County, fips, State, votes, total)) ### IF there is an error here. It would be bc "pct" needs to be removed. 

## save predictors and class labels
colors_county <- election_colors %>% 
  select(-c(County, fips, State, votes, total, first, second, percent_dif, candidate)) ### IF there is an error here. It would be bc "pct" needs to be removed. 
```

```{r, fig.height= 4, fig.width= 10}
### Constructing ridge plots 
# colors_county %>% 
#   names() %>% 
#   length()
# colors_county

colors_county %>%
  na.omit() %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  mutate_if(is.numeric, ~scale(., center = T, scale = T)) %>% 
  gather(key = 'variable', value = 'value', 1:26) %>%
  ggplot(aes(y = variable, x = value)) +
  geom_density_ridges(aes(fill = color), 
                      bandwidth = 0.2,
                      alpha = 0.5) +
  theme_minimal() +
  scale_fill_manual(values = c("red", "steelblue", "springgreen3")) + # Purple is not used as it is hard to see when red and blue overlap
  xlim(c(-3, 3)) +
  ggtitle("Ridge Plot Variable Distributions") + 
  labs(y = '')
```

Purple counties were chosen to be represented as green for the ridge plot as it would be difficult to distinguish purple from red and blue when they overlap. In general, we see that purple counties tend to have distributions between red and blue counties. This can be exemplified when looking at the transit variable, purple counties have a taller distribution than blue counties, but a shorted distribution than red counties. In general, there are very few variables were the distribution of purple counties stand out from the other two categories. The distribution for purple counties looks especially left skewed for the office variable.

```{r 3D_plot, fig.height= 4, fig.width= 10}
# Computing PCs 
col_x_mx <- colors_county %>% 
  select(-color) %>% 
  scale(center = T, scale = T) %>% 
  as.matrix()

the_colors <- colors_county$color

col_svd <- svd(col_x_mx)

col_z_mx <- col_x_mx %*% col_svd$v

# Constructing a scatterplot 
col_z_mx[, 1:2] %>% 
  as.data.frame() %>% 
  rename(PC1 = V1, PC2 = V2) %>% 
  bind_cols(the_colors) %>% 
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(aes(color = the_colors), alpha = 0.5) + 
  ggtitle("Scatterplot of Observations Across the First Two PCs") + 
  scale_color_manual(values = c("red", "steelblue", "springgreen3")) + 
  theme_bw()


# Trying a 3D plot 
# library(plotly)
# for_plot <- col_z_mx[, 1:3] %>% 
#   as.data.frame() %>% 
#   rename(PC1 = V1, PC2 = V2, PC3 = V3) %>% 
#   mutate(colors = the_colors)
# 
# plot_ly(for_plot, x = ~PC1, y = ~PC2, z = ~PC3, type = "scatter3d", mode = "markers", color = ~colors,
#         colors = "Set1")
```

Looking at the observations in respect to the first two principle components we see a similar story. We can somewhat distringuish red counties from blue counties - red counties are clustered in the middle while blue counties are more towards the periphery. But the purple counties - represnted in green are very spread out and difficult to group together.

```{r}
### KNN 
set.seed(420)
colors_scale <- colors_county %>% 
  mutate(across(-color, scale))

cv_data <- colors_scale %>% 
  crossv_kfold(k = 6, id = "fold")

# Copy and paste LOL 
k <- c(1, 5, 10, 15, 20, 25)

for (i in 1:6) {
  test <- colors_scale[cv_data$test[[i]]$idx, ] %>% 
    dplyr::select(-color) %>% 
    as.matrix()
  train <- colors_scale[cv_data$train[[i]]$idx, ] %>% 
    dplyr::select(-color) %>% 
    as.matrix()

  test_classes <- colors_scale[cv_data$test[[i]]$idx, ] %>% 
    dplyr::pull(color)
  train_classes <- colors_scale[cv_data$train[[i]]$idx, ] %>% 
    dplyr::pull(color)

  test_row_errors <- c()
  train_row_errors <- c()
  for (j in k) {
    test_pred <- knn(train, test, train_classes, j, prob = T)
    train_pred <- knn(train, train, train_classes, j, prob = T)
    
    test_tab <- table(test_pred, test_classes)
    train_tab <- table(train_pred, train_classes)
    
    test_error <- ((test_tab[1, 2] + test_tab[1, 3] + test_tab[2, 1] + test_tab[2, 3] + test_tab[3, 1] + test_tab[3, 2]) / length(test_classes))
    train_error <- ((train_tab[1, 2] + train_tab[1, 3] + train_tab[2, 1] + train_tab[2, 3] + train_tab[3, 1] + train_tab[3, 2]) /
                      length(train_classes))
    
    test_row_errors <- append(test_row_errors, test_error)
    train_row_errors <- append(train_row_errors, train_error)
  }
  
  errors <- c()
  errors <- c(test_row_errors, train_row_errors)
  
  if (i == 1) {
    test_row_bind <- as.data.frame(matrix(errors, 1, length(errors)))
  } else {
    test_row_bind <- rbind(test_row_bind, matrix(errors, 1, length(errors)))
  }
}

columns <- c('k1_test', 'k5_test', 'k10_test', 'k15_test', 'k20_test', 'k25_test',
             'k1_train', 'k5_train', 'k10_train', 'k15_train', 'k20_train', 'k25_train')
colnames(test_row_bind) <- columns

test_row_bind %>% 
  colMeans() %>%
  pander(caption = "Average Error over Folds: knn")
```

Training the data using k nearest neighbors, we see that the smallest test error occurs at a k of 15 and the smallest training error occurs at a k of 1. A larger k implies that states with similar features tend to vote in a similar fashion.

```{r}
# KNN continued 
without_colors <- colors_scale %>% 
  select(-color)

loocv_pred <- knn.cv(train = without_colors, cl = colors_scale$color, k = 1)
actual <- colors_scale$color

loocv_tab <- table(loocv_pred, actual)

(loocv_tab / rowSums(loocv_tab)) %>% 
  pander(caption = "True vs Predicted Error Rates: KNN LOOCV")
```

Applying LOOCV K nearest neighbors, we see that the correct prediction rate for red counties is relatively high 0.9323. While the correct prediction rate for blue counties is much lower at 0.6878 and the prediction for purple counties is almost useless 0.1119. 

In summary, purple counties were defined as those that had a voting percent differential of less than or equal to 4%. Looking at the variable distribution of the three categories, at appeared that purple counties tended to have features inbetween blue and purple counties. Visualing across the first two principle components, it could be observed that purple counties were very difficult to extricate from blue and especially red counties. Attempting to apply k nearest neighbors to the data confirmed this hunch as purple county accuracy was especially poor. 
